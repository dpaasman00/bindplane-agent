// Code generated by mdatagen. DO NOT EDIT.

package metadata

import (
	"time"

	"go.opentelemetry.io/collector/component"
	"go.opentelemetry.io/collector/confmap"
	"go.opentelemetry.io/collector/pdata/pcommon"
	"go.opentelemetry.io/collector/pdata/pmetric"
	"go.opentelemetry.io/collector/receiver"
)

// MetricSettings provides common settings for a particular metric.
type MetricSettings struct {
	Enabled bool `mapstructure:"enabled"`

	enabledSetByUser bool
}

func (ms *MetricSettings) Unmarshal(parser *confmap.Conf) error {
	if parser == nil {
		return nil
	}
	err := parser.Unmarshal(ms, confmap.WithErrorUnused())
	if err != nil {
		return err
	}
	ms.enabledSetByUser = parser.IsSet("enabled")
	return nil
}

// MetricsSettings provides settings for apachedruidreceiver metrics.
type MetricsSettings struct {
	ApachedruidAverageSQLQueryBytes  MetricSettings `mapstructure:"apachedruid.average_sql_query_bytes"`
	ApachedruidAverageSQLQueryTime   MetricSettings `mapstructure:"apachedruid.average_sql_query_time"`
	ApachedruidFailedQueryCount      MetricSettings `mapstructure:"apachedruid.failed_query_count"`
	ApachedruidInterruptedQueryCount MetricSettings `mapstructure:"apachedruid.interrupted_query_count"`
	ApachedruidQueryCount            MetricSettings `mapstructure:"apachedruid.query_count"`
	ApachedruidSQLQueryCount         MetricSettings `mapstructure:"apachedruid.sql_query_count"`
	ApachedruidSuccessQueryCount     MetricSettings `mapstructure:"apachedruid.success_query_count"`
	ApachedruidTimeoutQueryCount     MetricSettings `mapstructure:"apachedruid.timeout_query_count"`
}

func DefaultMetricsSettings() MetricsSettings {
	return MetricsSettings{
		ApachedruidAverageSQLQueryBytes: MetricSettings{
			Enabled: true,
		},
		ApachedruidAverageSQLQueryTime: MetricSettings{
			Enabled: true,
		},
		ApachedruidFailedQueryCount: MetricSettings{
			Enabled: true,
		},
		ApachedruidInterruptedQueryCount: MetricSettings{
			Enabled: true,
		},
		ApachedruidQueryCount: MetricSettings{
			Enabled: true,
		},
		ApachedruidSQLQueryCount: MetricSettings{
			Enabled: true,
		},
		ApachedruidSuccessQueryCount: MetricSettings{
			Enabled: true,
		},
		ApachedruidTimeoutQueryCount: MetricSettings{
			Enabled: true,
		},
	}
}

// ResourceAttributeSettings provides common settings for a particular metric.
type ResourceAttributeSettings struct {
	Enabled bool `mapstructure:"enabled"`
}

// ResourceAttributesSettings provides settings for apachedruidreceiver metrics.
type ResourceAttributesSettings struct {
	ApachedruidService ResourceAttributeSettings `mapstructure:"apachedruid.service"`
}

func DefaultResourceAttributesSettings() ResourceAttributesSettings {
	return ResourceAttributesSettings{
		ApachedruidService: ResourceAttributeSettings{
			Enabled: true,
		},
	}
}

type metricApachedruidAverageSQLQueryBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.average_sql_query_bytes metric with initial data.
func (m *metricApachedruidAverageSQLQueryBytes) init() {
	m.data.SetName("apachedruid.average_sql_query_bytes")
	m.data.SetDescription("The average number of bytes returned by a SQL query.")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidAverageSQLQueryBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, dataSourceAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("data_source", dataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidAverageSQLQueryBytes) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidAverageSQLQueryBytes) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidAverageSQLQueryBytes(settings MetricSettings) metricApachedruidAverageSQLQueryBytes {
	m := metricApachedruidAverageSQLQueryBytes{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidAverageSQLQueryTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.average_sql_query_time metric with initial data.
func (m *metricApachedruidAverageSQLQueryTime) init() {
	m.data.SetName("apachedruid.average_sql_query_time")
	m.data.SetDescription("The average number of milliseconds taken to complete a SQL query.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidAverageSQLQueryTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, dataSourceAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("data_source", dataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidAverageSQLQueryTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidAverageSQLQueryTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidAverageSQLQueryTime(settings MetricSettings) metricApachedruidAverageSQLQueryTime {
	m := metricApachedruidAverageSQLQueryTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidFailedQueryCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.failed_query_count metric with initial data.
func (m *metricApachedruidFailedQueryCount) init() {
	m.data.SetName("apachedruid.failed_query_count")
	m.data.SetDescription("Total number of failed queries since the previous data point.")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricApachedruidFailedQueryCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidFailedQueryCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidFailedQueryCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidFailedQueryCount(settings MetricSettings) metricApachedruidFailedQueryCount {
	m := metricApachedruidFailedQueryCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidInterruptedQueryCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.interrupted_query_count metric with initial data.
func (m *metricApachedruidInterruptedQueryCount) init() {
	m.data.SetName("apachedruid.interrupted_query_count")
	m.data.SetDescription("Total number of interrupted queries since the previous data point.")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricApachedruidInterruptedQueryCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidInterruptedQueryCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidInterruptedQueryCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidInterruptedQueryCount(settings MetricSettings) metricApachedruidInterruptedQueryCount {
	m := metricApachedruidInterruptedQueryCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidQueryCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.query_count metric with initial data.
func (m *metricApachedruidQueryCount) init() {
	m.data.SetName("apachedruid.query_count")
	m.data.SetDescription("Total number of queries executed since the previous data point.")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricApachedruidQueryCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidQueryCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidQueryCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidQueryCount(settings MetricSettings) metricApachedruidQueryCount {
	m := metricApachedruidQueryCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSQLQueryCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.sql_query_count metric with initial data.
func (m *metricApachedruidSQLQueryCount) init() {
	m.data.SetName("apachedruid.sql_query_count")
	m.data.SetDescription("Total number of SQL queries executed since the previous data point.")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricApachedruidSQLQueryCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, dataSourceAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("data_source", dataSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSQLQueryCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSQLQueryCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSQLQueryCount(settings MetricSettings) metricApachedruidSQLQueryCount {
	m := metricApachedruidSQLQueryCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidSuccessQueryCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.success_query_count metric with initial data.
func (m *metricApachedruidSuccessQueryCount) init() {
	m.data.SetName("apachedruid.success_query_count")
	m.data.SetDescription("Total number of successful queries since the previous data point.")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricApachedruidSuccessQueryCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidSuccessQueryCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidSuccessQueryCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidSuccessQueryCount(settings MetricSettings) metricApachedruidSuccessQueryCount {
	m := metricApachedruidSuccessQueryCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricApachedruidTimeoutQueryCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills apachedruid.timeout_query_count metric with initial data.
func (m *metricApachedruidTimeoutQueryCount) init() {
	m.data.SetName("apachedruid.timeout_query_count")
	m.data.SetDescription("Total number of timed out queries since the previous data point.")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricApachedruidTimeoutQueryCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricApachedruidTimeoutQueryCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricApachedruidTimeoutQueryCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricApachedruidTimeoutQueryCount(settings MetricSettings) metricApachedruidTimeoutQueryCount {
	m := metricApachedruidTimeoutQueryCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

// MetricsBuilderConfig is a structural subset of an otherwise 1-1 copy of metadata.yaml
type MetricsBuilderConfig struct {
	Metrics            MetricsSettings            `mapstructure:"metrics"`
	ResourceAttributes ResourceAttributesSettings `mapstructure:"resource_attributes"`
}

// MetricsBuilder provides an interface for scrapers to report metrics while taking care of all the transformations
// required to produce metric representation defined in metadata and user settings.
type MetricsBuilder struct {
	startTime                              pcommon.Timestamp   // start time that will be applied to all recorded data points.
	metricsCapacity                        int                 // maximum observed number of metrics per resource.
	resourceCapacity                       int                 // maximum observed number of resource attributes.
	metricsBuffer                          pmetric.Metrics     // accumulates metrics data before emitting.
	buildInfo                              component.BuildInfo // contains version information
	resourceAttributesSettings             ResourceAttributesSettings
	metricApachedruidAverageSQLQueryBytes  metricApachedruidAverageSQLQueryBytes
	metricApachedruidAverageSQLQueryTime   metricApachedruidAverageSQLQueryTime
	metricApachedruidFailedQueryCount      metricApachedruidFailedQueryCount
	metricApachedruidInterruptedQueryCount metricApachedruidInterruptedQueryCount
	metricApachedruidQueryCount            metricApachedruidQueryCount
	metricApachedruidSQLQueryCount         metricApachedruidSQLQueryCount
	metricApachedruidSuccessQueryCount     metricApachedruidSuccessQueryCount
	metricApachedruidTimeoutQueryCount     metricApachedruidTimeoutQueryCount
}

// metricBuilderOption applies changes to default metrics builder.
type metricBuilderOption func(*MetricsBuilder)

// WithStartTime sets startTime on the metrics builder.
func WithStartTime(startTime pcommon.Timestamp) metricBuilderOption {
	return func(mb *MetricsBuilder) {
		mb.startTime = startTime
	}
}

func DefaultMetricsBuilderConfig() MetricsBuilderConfig {
	return MetricsBuilderConfig{
		Metrics:            DefaultMetricsSettings(),
		ResourceAttributes: DefaultResourceAttributesSettings(),
	}
}

func NewMetricsBuilderConfig(ms MetricsSettings, ras ResourceAttributesSettings) MetricsBuilderConfig {
	return MetricsBuilderConfig{
		Metrics:            ms,
		ResourceAttributes: ras,
	}
}

func NewMetricsBuilder(mbc MetricsBuilderConfig, settings receiver.CreateSettings, options ...metricBuilderOption) *MetricsBuilder {
	mb := &MetricsBuilder{
		startTime:                              pcommon.NewTimestampFromTime(time.Now()),
		metricsBuffer:                          pmetric.NewMetrics(),
		buildInfo:                              settings.BuildInfo,
		resourceAttributesSettings:             mbc.ResourceAttributes,
		metricApachedruidAverageSQLQueryBytes:  newMetricApachedruidAverageSQLQueryBytes(mbc.Metrics.ApachedruidAverageSQLQueryBytes),
		metricApachedruidAverageSQLQueryTime:   newMetricApachedruidAverageSQLQueryTime(mbc.Metrics.ApachedruidAverageSQLQueryTime),
		metricApachedruidFailedQueryCount:      newMetricApachedruidFailedQueryCount(mbc.Metrics.ApachedruidFailedQueryCount),
		metricApachedruidInterruptedQueryCount: newMetricApachedruidInterruptedQueryCount(mbc.Metrics.ApachedruidInterruptedQueryCount),
		metricApachedruidQueryCount:            newMetricApachedruidQueryCount(mbc.Metrics.ApachedruidQueryCount),
		metricApachedruidSQLQueryCount:         newMetricApachedruidSQLQueryCount(mbc.Metrics.ApachedruidSQLQueryCount),
		metricApachedruidSuccessQueryCount:     newMetricApachedruidSuccessQueryCount(mbc.Metrics.ApachedruidSuccessQueryCount),
		metricApachedruidTimeoutQueryCount:     newMetricApachedruidTimeoutQueryCount(mbc.Metrics.ApachedruidTimeoutQueryCount),
	}
	for _, op := range options {
		op(mb)
	}
	return mb
}

// updateCapacity updates max length of metrics and resource attributes that will be used for the slice capacity.
func (mb *MetricsBuilder) updateCapacity(rm pmetric.ResourceMetrics) {
	if mb.metricsCapacity < rm.ScopeMetrics().At(0).Metrics().Len() {
		mb.metricsCapacity = rm.ScopeMetrics().At(0).Metrics().Len()
	}
	if mb.resourceCapacity < rm.Resource().Attributes().Len() {
		mb.resourceCapacity = rm.Resource().Attributes().Len()
	}
}

// ResourceMetricsOption applies changes to provided resource metrics.
type ResourceMetricsOption func(ResourceAttributesSettings, pmetric.ResourceMetrics)

// WithApachedruidService sets provided value as "apachedruid.service" attribute for current resource.
func WithApachedruidService(val string) ResourceMetricsOption {
	return func(ras ResourceAttributesSettings, rm pmetric.ResourceMetrics) {
		if ras.ApachedruidService.Enabled {
			rm.Resource().Attributes().PutStr("apachedruid.service", val)
		}
	}
}

// WithStartTimeOverride overrides start time for all the resource metrics data points.
// This option should be only used if different start time has to be set on metrics coming from different resources.
func WithStartTimeOverride(start pcommon.Timestamp) ResourceMetricsOption {
	return func(ras ResourceAttributesSettings, rm pmetric.ResourceMetrics) {
		var dps pmetric.NumberDataPointSlice
		metrics := rm.ScopeMetrics().At(0).Metrics()
		for i := 0; i < metrics.Len(); i++ {
			switch metrics.At(i).Type() {
			case pmetric.MetricTypeGauge:
				dps = metrics.At(i).Gauge().DataPoints()
			case pmetric.MetricTypeSum:
				dps = metrics.At(i).Sum().DataPoints()
			}
			for j := 0; j < dps.Len(); j++ {
				dps.At(j).SetStartTimestamp(start)
			}
		}
	}
}

// EmitForResource saves all the generated metrics under a new resource and updates the internal state to be ready for
// recording another set of data points as part of another resource. This function can be helpful when one scraper
// needs to emit metrics from several resources. Otherwise calling this function is not required,
// just `Emit` function can be called instead.
// Resource attributes should be provided as ResourceMetricsOption arguments.
func (mb *MetricsBuilder) EmitForResource(rmo ...ResourceMetricsOption) {
	rm := pmetric.NewResourceMetrics()
	rm.Resource().Attributes().EnsureCapacity(mb.resourceCapacity)
	ils := rm.ScopeMetrics().AppendEmpty()
	ils.Scope().SetName("otelcol/apachedruidreceiver")
	ils.Scope().SetVersion(mb.buildInfo.Version)
	ils.Metrics().EnsureCapacity(mb.metricsCapacity)
	mb.metricApachedruidAverageSQLQueryBytes.emit(ils.Metrics())
	mb.metricApachedruidAverageSQLQueryTime.emit(ils.Metrics())
	mb.metricApachedruidFailedQueryCount.emit(ils.Metrics())
	mb.metricApachedruidInterruptedQueryCount.emit(ils.Metrics())
	mb.metricApachedruidQueryCount.emit(ils.Metrics())
	mb.metricApachedruidSQLQueryCount.emit(ils.Metrics())
	mb.metricApachedruidSuccessQueryCount.emit(ils.Metrics())
	mb.metricApachedruidTimeoutQueryCount.emit(ils.Metrics())

	for _, op := range rmo {
		op(mb.resourceAttributesSettings, rm)
	}
	if ils.Metrics().Len() > 0 {
		mb.updateCapacity(rm)
		rm.MoveTo(mb.metricsBuffer.ResourceMetrics().AppendEmpty())
	}
}

// Emit returns all the metrics accumulated by the metrics builder and updates the internal state to be ready for
// recording another set of metrics. This function will be responsible for applying all the transformations required to
// produce metric representation defined in metadata and user settings, e.g. delta or cumulative.
func (mb *MetricsBuilder) Emit(rmo ...ResourceMetricsOption) pmetric.Metrics {
	mb.EmitForResource(rmo...)
	metrics := mb.metricsBuffer
	mb.metricsBuffer = pmetric.NewMetrics()
	return metrics
}

// RecordApachedruidAverageSQLQueryBytesDataPoint adds a data point to apachedruid.average_sql_query_bytes metric.
func (mb *MetricsBuilder) RecordApachedruidAverageSQLQueryBytesDataPoint(ts pcommon.Timestamp, val float64, dataSourceAttributeValue string) {
	mb.metricApachedruidAverageSQLQueryBytes.recordDataPoint(mb.startTime, ts, val, dataSourceAttributeValue)
}

// RecordApachedruidAverageSQLQueryTimeDataPoint adds a data point to apachedruid.average_sql_query_time metric.
func (mb *MetricsBuilder) RecordApachedruidAverageSQLQueryTimeDataPoint(ts pcommon.Timestamp, val float64, dataSourceAttributeValue string) {
	mb.metricApachedruidAverageSQLQueryTime.recordDataPoint(mb.startTime, ts, val, dataSourceAttributeValue)
}

// RecordApachedruidFailedQueryCountDataPoint adds a data point to apachedruid.failed_query_count metric.
func (mb *MetricsBuilder) RecordApachedruidFailedQueryCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidFailedQueryCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidInterruptedQueryCountDataPoint adds a data point to apachedruid.interrupted_query_count metric.
func (mb *MetricsBuilder) RecordApachedruidInterruptedQueryCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidInterruptedQueryCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidQueryCountDataPoint adds a data point to apachedruid.query_count metric.
func (mb *MetricsBuilder) RecordApachedruidQueryCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidQueryCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidSQLQueryCountDataPoint adds a data point to apachedruid.sql_query_count metric.
func (mb *MetricsBuilder) RecordApachedruidSQLQueryCountDataPoint(ts pcommon.Timestamp, val int64, dataSourceAttributeValue string) {
	mb.metricApachedruidSQLQueryCount.recordDataPoint(mb.startTime, ts, val, dataSourceAttributeValue)
}

// RecordApachedruidSuccessQueryCountDataPoint adds a data point to apachedruid.success_query_count metric.
func (mb *MetricsBuilder) RecordApachedruidSuccessQueryCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidSuccessQueryCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordApachedruidTimeoutQueryCountDataPoint adds a data point to apachedruid.timeout_query_count metric.
func (mb *MetricsBuilder) RecordApachedruidTimeoutQueryCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricApachedruidTimeoutQueryCount.recordDataPoint(mb.startTime, ts, val)
}

// Reset resets metrics builder to its initial state. It should be used when external metrics source is restarted,
// and metrics builder should update its startTime and reset it's internal state accordingly.
func (mb *MetricsBuilder) Reset(options ...metricBuilderOption) {
	mb.startTime = pcommon.NewTimestampFromTime(time.Now())
	for _, op := range options {
		op(mb)
	}
}
